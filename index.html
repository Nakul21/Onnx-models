<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Text Extract</title>
    <style>
        #timer {
            font-size: 1.5em;
            margin-top: 10px;
        }
    </style>
</head>
<body>
    <input type="file" id="imageUpload" accept="image/*">
    <img id="imagePreview" alt="Image Preview" style="max-width: 100%; max-height: 300px; margin-top: 10px;">
    <button id="extractButton" style="margin-top: 10px;">Extract Text</button>
    <div id="timer">00:00:00</div>
    <canvas id="canvas" style="display: none;"></canvas>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/opencv.js"></script>
    <script>
        let startTime;
        let timerInterval;

        function startTimer() {
            startTime = new Date();
            timerInterval = setInterval(updateTimer, 1000);
        }

        function stopTimer() {
            clearInterval(timerInterval);
        }

        function updateTimer() {
            const currentTime = new Date();
            const timeDifference = currentTime - startTime;
            const seconds = Math.floor(timeDifference / 1000) % 60;
            const minutes = Math.floor(timeDifference / (1000 * 60)) % 60;
            const hours = Math.floor(timeDifference / (1000 * 60 * 60));
            document.getElementById('timer').innerText = `${pad(hours)}:${pad(minutes)}:${pad(seconds)}`;
        }

        function pad(value) {
            return value < 10 ? `0${value}` : value;
        }

        async function runDetectionModel(imageData) {
            try {
                const session = await ort.InferenceSession.create('models/rep_fast_base.onnx');
                const image = cv.imdecode(imageData, cv.IMREAD_COLOR);
                const inputTensor = preprocessImageForDetection(image); // Preprocess the image as required by the detection model
                const feeds = { 'input': inputTensor };
                const results = await session.run(feeds);
                return postprocessDetectionResults(results); // Postprocess the results to get bounding boxes
            } catch (error) {
                console.error('Error running detection model:', error);
                return [];
            }
        }

        async function runRecognitionModel(croppedImages) {
            try {
                const session = await ort.InferenceSession.create('models/parseq_dynamic.onnx');
                const results = [];
                for (const croppedImage of croppedImages) {
                    const inputTensor = preprocessImageForRecognition(croppedImage); // Preprocess the cropped image as required by the recognition model
                    const feeds = { 'input': inputTensor };
                    const result = await session.run(feeds);
                    results.push(postprocessRecognitionResult(result)); // Postprocess the result to get recognized text
                }
                return results;
            } catch (error) {
                console.error('Error running recognition model:', error);
                return [];
            }
        }

        function preprocessImageForDetection(image) {
            const resizedImage = new cv.Mat();
            cv.resize(image, resizedImage, new cv.Size(1024, 1024), 0, 0, cv.INTER_AREA);
            const normalizedImage = new cv.Mat();
            resizedImage.convertTo(normalizedImage, cv.CV_32F, 1.0 / 255.0);
            const data = new Float32Array(normalizedImage.data32F);
            return new ort.Tensor('float32', data, [1, 3, 1024, 1024]); // Convert to ONNX tensor
        }

        function preprocessImageForRecognition(image) {
            const resizedImage = new cv.Mat();
            cv.resize(image, resizedImage, new cv.Size(128, 32), 0, 0, cv.INTER_AREA);
            const normalizedImage = new cv.Mat();
            resizedImage.convertTo(normalizedImage, cv.CV_32F, 1.0 / 255.0);
            const data = new Float32Array(normalizedImage.data32F);
            return new ort.Tensor('float32', data, [1, 3, 32, 128]); // Convert to ONNX tensor
        }

        function postprocessDetectionResults(results) {
            const boundingBoxes = results['output'].data.map(box => ({
                x: box[0],
                y: box[1],
                width: box[2] - box[0],
                height: box[3] - box[1]
            }));
            return boundingBoxes;
        }

        function postprocessRecognitionResult(result) {
            const text = result['output'].data.map(charCode => String.fromCharCode(charCode)).join('');
            return text;
        }

        async function main(file) {
            const reader = new FileReader();
            reader.onload = async (event) => {
                const imageData = new Uint8Array(event.target.result);
                const boundingBoxes = await runDetectionModel(imageData);
                const image = cv.imdecode(imageData, cv.IMREAD_COLOR);
                const croppedImages = boundingBoxes.map(box => {
                    const roi = image.roi(new cv.Rect(box.x, box.y, box.width, box.height));
                    const cropped = new cv.Mat();
                    roi.copyTo(cropped);
                    return cropped;
                });
                const recognizedTexts = await runRecognitionModel(croppedImages);
                console.log(recognizedTexts);
                stopTimer();
                alert(`OCR Results:\n${recognizedTexts.join('\n')}`);
            };
            reader.readAsArrayBuffer(file);

            // Set the image preview
            const imgPreview = document.getElementById('imagePreview');
            imgPreview.src = URL.createObjectURL(file);
        }

        document.getElementById('imageUpload').addEventListener('change', (event) => {
            const file = event.target.files[0];
            if (file) {
                main(file);
            } else {
                console.log('No file selected');
            }
        });

        document.getElementById('extractButton').addEventListener('click', () => {
            const file = document.getElementById('imageUpload').files[0];
            if (file) {
                startTimer();
                main(file);
            } else {
                alert('Please select an image first.');
            }
        });

        // Initialize OpenCV
        function onOpenCvReady() {
            console.log('OpenCV.js is ready');
        }
        onOpenCvReady();

    </script>
</body>
</html>
